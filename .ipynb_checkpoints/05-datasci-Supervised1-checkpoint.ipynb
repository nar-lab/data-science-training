{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVANCED ANALYTICS 2018\n",
    "_Ali Ã‡abukel - Gtech_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "- When you have a result variable or outcome like match scores, house prices, sentiment labels in the historical data, you can find a significant model, which can be a formula or rule form, for classification and prediction.\n",
    "- The fitted model can be interpretable or black-box, also it is used to score for future events.\n",
    "- The model's success can be measured in the training phase by splitting the data and rule-of-thumbs for splitting portions as follows:\n",
    "    + Training dataset can be useed for fitting model, %80 of population \n",
    "    + Testing dataset can be used for selecting golden model, %20 of population \n",
    "    + Validation dataset can be used for validating the golden model, %20 of population \n",
    "    + The main idea is to proof the sucess of model in the several data samples (out-of-sample) and time periods (out-of-date)\n",
    "- The concept of bias-variance tradeoff is very important strategy for deciding model complexity, number of experiments and regularization techniques\n",
    "- The importance of the predictors or input variables can be calculated for target variable, so feature elimination can be applied before training. Also, there is some metrics which can be used for automated model and feature selection: Akaike, Bayesian Information Criterion.\n",
    "- The general performance measures: Confussion Matrix, ROC (AUC-Gini), Cumulative Gain-Lift Chart, R-Squared, RMSE,  F-score\n",
    "    + __All models are wrong__, you can find a better one than the current model\n",
    "    + Model performance can not show the best performance (%100). The main objective is to find a meaningful model in terms of business, also sufficient convergence and stable performance in time\n",
    "- The general cross-validation and sampling techniques: Hold-out, Random sampling, K-fold, Bootstrap, LOOCV\n",
    "- The popular algorithms:\n",
    "    + Regression: Linear, Polynomial, Logistic\n",
    "    + k-Nearest Neighbors\n",
    "    + Decision Trees: Simple, bagging, boosting\n",
    "    + Naive Bayes\n",
    "    + Support Vector Machines\n",
    "    + Neural Networks\n",
    "- Ensemble modelling: Many different models, include different variables, can be used in the production at the same time. So, you can increase the prediction probability score for the decision making process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
