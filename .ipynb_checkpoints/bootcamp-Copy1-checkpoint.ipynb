{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVANCED ANALYTICS 2018\n",
    "- Basic Concepts\n",
    "    + Business Intelligence vs. Advanced Analytics\n",
    "    + Traditional Computing vs. Machine Learning\n",
    "    + Evolution of Machine Learning\n",
    "    + Types of Business Analytics\n",
    "    + Data Science Definition and Skillset\n",
    "    + Data Science Puzzle\n",
    "    + Short Definitions\n",
    "    + Artificial Intelligence\n",
    "    + Deep Machine Learning\n",
    "    + Machine Learning 3.0\n",
    "    + Big Data Landscape 2018\n",
    "- Advanced Analytics Lifecycle\n",
    "    + Solution Building Lifecycle\n",
    "    + Machine Learning Lifecycle\n",
    "- Common Business Problems\n",
    "    + Potential Problems\n",
    "- Types of Machine Learning\n",
    "    + Learning Types\n",
    "- Rich Data Formats\n",
    "    + New Generation Data\n",
    "    + Diverse Data Sources\n",
    "    + Structured Data Shape and Feature Roles\n",
    "- Data Preparation\n",
    "    + ETL and Data Pre-processing\n",
    "    + Data Quality and Missing Values\n",
    "    + One-hot Encoding\n",
    "- Exploratory Data Analysis and Feature Engineering\n",
    "    + Gaussian Distribution and Histogram\n",
    "    + Inter-Quantile Range and Boxplot\n",
    "    + Standardization and Normalization\n",
    "    + Discretization and Binning\n",
    "    + Dimensionality Reduction and Feature Extraction\n",
    "    + Principal Component Analysis\n",
    "    + Singular Value Decomposition\n",
    "    + Non-negative Matrix Factorization\n",
    "    + Feature Derivation\n",
    "    + Predictive Modelling and Data Leakage\n",
    "- Supervised Learning\n",
    "    + Regression\n",
    "    + Linear Regression Hands-on solution\n",
    "    + Gradient Descent\n",
    "    + Logistic Regression\n",
    "    + Evaluation of Classification\n",
    "    + Bias-Variance Tradeoff\n",
    "    + Regularization, Feature-Model Selection\n",
    "    + Perceptron\n",
    "    + Multi-layer Perceptron\n",
    "    + Feed Forward Neural Networks\n",
    "    + Backpropogration\n",
    "    + Deep Neural Networks\n",
    "    + Naive Bayes\n",
    "    + Decision Trees\n",
    "    + Support Vector Machines\n",
    "    + k-Nearest Neighbors\n",
    "- Unsupervised Learning\n",
    "    + K-means Clustering\n",
    "    + Cosine Simiilarity\n",
    "    + Density Based Scan Clustering\n",
    "    + Expectation Maximization\n",
    "    + Orthogonal Partitioning\n",
    "    + Anomaly Detection - One-class SVM\n",
    "    + Apriori\n",
    "    + Collaborative Filtering\n",
    "    + Social Network Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Intelligence vs. Advanced Analytics\n",
    "- Traditional BI interests in the questions using past events.\n",
    "- Advanced Analytics try to find the reasons of events, also extract an equation or rule set to cacth next events using historical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Image1\"](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Computing vs. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Traditional Programming vs. Machine Learning](2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evo of ML](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Business Analytics\n",
    "+ Descriptive Analytics\n",
    "+ Diagnostic Analytics\n",
    "+ __Predictive Analytics__\n",
    "+ Prescriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of BA](4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of BA](5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Definiton and Skillset\n",
    "Evey __Data Scientist__ should be specialized in the following areas:\n",
    "- Math and Statistics\n",
    "- Domain Knowledge\n",
    "- Computing Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Science Definition](6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Scientist Skill Set](7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Science Puzzle](8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Quick Definitions](9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence\n",
    "- Artificial Intelligence focuses on robotics and machines which have human functions like understanding what it sees, listens and also reactions.\n",
    "- Machine Learning means the theories of learning experiments from data.\n",
    "- Deep Learning's main objective is to process language, images, audios using deep, huge neural networks as human brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AI, ML and DL](10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Machine Learning\n",
    "We need to extract new features and informations while creating model manually. Deep Learning will provide automated feature extraction in the modelling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning](11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning 3.0](12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Landscape 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Big Data Landscape 2018](13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analytics Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Building Process\n",
    "The process include the following components:\n",
    "- Business Problem Formulation\n",
    "- Data Science\n",
    "- Business Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Solution Building Process](14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Lifecycle\n",
    "\n",
    "Cross-Industry Standart Process for Data Mining (CRISP-DM):\n",
    "- Business Understanding\n",
    "- Data Understandig\n",
    "- Data Preparation\n",
    "- Modelling \n",
    "- Evaluation\n",
    "- Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML Lifecycle](15.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLLC1](16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLLC2](17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLLC3](18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLLC4](19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLLC5](20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Business Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Problems\n",
    "\n",
    "https://medium.mybridge.co/30-amazing-machine-learning-projects-for-the-past-year-v-2018-b853b8621ac7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BP1](21.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BP2](22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BP3](23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BP4](24.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Types\n",
    "Machine learning has mainly 3 different learning techniques as follows:\n",
    "- Supervised Learning is a common machine learning technique. It can be applied when/if data has target variable, labeled training set or result. The task is appropriate to detect spam emails or to predict house prices in the future. \n",
    "    + Classificiton\n",
    "    + Regression\n",
    "    \n",
    "- Unsupervised Learning can be used for exploratory data analysis. Training dataset and model performance measurement are not generally useful for this task. You can find out your customer groups/clusters or recommend new products to your customers using unsupervised learning techniques.\n",
    "    + Clustering\n",
    "    + Association\n",
    "    \n",
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ml1](25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML2](26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML3](27.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML4](28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML5](29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML6](30.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML7](31.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rich Data Formats\n",
    "- Structured\n",
    "    + Tabular Dataaset: RDBMS, Delimited Files, Matrix Formats, Transactional\n",
    "    + Graph Data: WWW, Connected Data (Property Graphs, Semantic Graphs)\n",
    "    + Databases Tables, CSV, DSV, FWF\n",
    "    + Sequential: Video - Audio Stream, Timeseries, Genom, Sensors, Clickstream\n",
    "    + Spatial: GIS, Images\n",
    "- Semi-structured\n",
    "    + Web Services, SOA, Social Media APIs\n",
    "    + JSON, XML\n",
    "- Unstructured\n",
    "    + Email, Conversation Data, Social Media Posts\n",
    "    + Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of Data](32.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Generation Data\n",
    "Enterprises and companies have new generation, valuable external data sources which enrich their internal datasets. In terms of technical view, companies can need to move their workloads to scalable and cheaper Big Data Systems.\n",
    "- Features of Big Data - 6V:\n",
    "    + Volume: Large scale (Peta, Zetta)\n",
    "    + Variety: Various Data Formats\n",
    "    + Velocity: Fast Data\n",
    "    + Veracity: Uncertain Data\n",
    "    + Valence: Linked Data\n",
    "    + __Value__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![New Generation Data](33.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5V](34.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diverse Data Sources\n",
    "- Human-generated: Social media conversations, Emails, Ratings, Applications....\n",
    "- Machine-generated: Sensors, Web-logs, GPS..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Formats](35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Data Shape and Feature Roles\n",
    "- Observations: Rows\n",
    "- Features, Attributes, Variables, Predictors: Columns\n",
    "- Target, Class, Label: Training Data\n",
    "- Identifier: Primary Key\n",
    "- Analytical Datamart should have the following properties:\n",
    "    + Clean, Well-Qualified, No noise\n",
    "    + Structured, Denormalized, 360-Degree View\n",
    "    + Unique Record for each Identifier, Customer ID, Account ID, Transaction ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Shape](36.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Roles of Data](37.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- Extract data from data sources, transform data to denormalized model, clean, sparse data and load as an Analytical Datamart\n",
    "- You can use the following tools and functions while data preparation:\n",
    "    + SQL and SQL Analytical & Window Functions\n",
    "    + Lambda expressions using functional programming like Python, R... (map, apply, filter, reduce...)\n",
    "    + Functions like reshape, One-Hot Encoding, pivot, transpose, split, union...\n",
    "    + Includes numerical and categorical features\n",
    "    + Spatial and geometric functions for location-based data\n",
    "    + Missing value treatment and imputation: mean, median, analytical model based\n",
    "    + For further reading: https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL and Data Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![etl](38.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![denormalized](39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality and Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![garbage](40.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![missing](41.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![missing](42.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ohe](43.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ohe2](44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Feature Engineering\n",
    "- Data Exploration includes the following categories and techniques:\n",
    "    + Univariate Analysis\n",
    "        - Visual: Histogram, Boxplots, Bar Charts, Line Charts\n",
    "        - Descriptive Statistics: Count, Distinct Count, Mean-Median-Mode, Sum, Min-Max Values, Standart Deviation-Variance\n",
    "        - Can be chosen proper techniqes across analytical data types which are numerical, categorical\n",
    "        - Provided insights from data distributions: central tendancy, dispersion, outliers-extremes \n",
    "    + Bivariate Analysis\n",
    "        - Relations: Covariance, Correlations, Contingency Matrix, Chi-Square, Entropy\n",
    "        - Scatterplots, Stacked Bar Charts, Grouped Bar-Charts, Pareto\n",
    "        - Correlation does not imply causality, it gives information about relation.\n",
    "        - Correlation coeffitions takes values from -1 (negative relation) to +1 (positive relation)\n",
    "        - Can be chosen 0.6 (absolute threshold) to define strong relation\n",
    "    + Multivariate Analysis\n",
    "- Data Pre-processing\n",
    "    + Normalization, Standardization: Min-max scaling, Z-score, Linear scale\n",
    "    + Discretazion, Binning: Equi-width, Percentile, Optimal Binning\n",
    "    + Outlier Detection: Edge values, 3rd standart deviation, 5th percentile\n",
    "- Feature Extraction and Dimensionality Reduction\n",
    "    + Matrix factorization techniques can be used for indicating loss of information, reducing dimensions easy to use, also selecting features in correlated features\n",
    "    + Can be applied on sparse data to reduce high dimensionality\n",
    "    + Principal Component Analysis, Singular Value Decomposition, Non-negative Matrix Factorization\n",
    "    + Sparse matrix examples: TF-IDF, Collaborative Filtering User-Item Matrix... \n",
    "- Feature Derivation\n",
    "    + Time based aggregated measures (Last 3-12 month, periods), ratios, relative variables\n",
    "- Predictive Modelling \n",
    "    + Period Design, Timeline, Observation Period, Prediction Period\n",
    "    + Data Leakage Problem\n",
    "    \n",
    "There is some popular frameworks for data visualization, namely ggplot2, Plot.ly, matplotlib, seaborn, D3.js...\n",
    "\n",
    "Common Probability Distributions: http://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual](45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eda](46.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![analysis types](48.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![distributions](47.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution and Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gauss](47.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histograms](48.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skewness](49.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kurtosis](50.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Quantile Range and Boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iqr](52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![box1](53.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![box2](54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rho](55.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relation level](56.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![direction](57.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![corr types](58.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization and Normalization\n",
    "\n",
    "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scaling1](59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scaling2](60.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scaling3](61.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![binning](62.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction and Feature Extraction\n",
    "\n",
    "Main usages:\n",
    "\n",
    "- Component/latent feature extraction\n",
    "- Data Compression\n",
    "- Data understanding, bivariate analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dimensionality](63.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "http://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "\n",
    "http://www.dataperspective.info/2016/02/principal-component-analysis-using-r.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pca1](139.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pca2](140.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pca3](141.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pca4](142.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svd1](143.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svd2](144.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svd3](145.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nmf1](146.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nmf2](147.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![derive](64.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modelling and Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![predictive](65.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![leakage](66.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and Lambda Hands-on-lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools and Techniques\n",
    "- Data Preparation: ETL, Pre-processing, Data Engineering using SQL\n",
    "    + PostgreSQL, Oracle Database Systems\n",
    "    + SQL Analytical & Window Functions\n",
    "    + SQL Statistical Functions\n",
    "    + SQL Pattern Matching\n",
    "    + SQL Hierarchical Queries\n",
    "    + Oracle Advanced Analytics includes Oracle Data Miner, Oracle R Enterprise\n",
    "    + Oracle Spatial & Graph\n",
    "    + Oracle Nested Columns\n",
    "    + PostGIS\n",
    "- Functional Programming and Lambda Expressions with Python\n",
    "    + Map, Filter, Reduce\n",
    "    + Anaconda Python 3, pandas, numpy\n",
    "    + Useful Functions\n",
    "-  Data Visualization\n",
    "    + R ggplot2\n",
    "    + Python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "- When you have a result variable or outcome like match scores, house prices, sentiment labels in the historical data, you can find a significant model, which can be a formula or rule form, for classification and prediction.\n",
    "- The fitted model can be interpretable or black-box, also it is used to score for future events.\n",
    "- The model's success can be measured in the training phase by splitting the data and rule-of-thumbs for splitting portions as follows:\n",
    "    + Training dataset can be useed for fitting model, %80 of population \n",
    "    + Testing dataset can be used for selecting golden model, %20 of population \n",
    "    + Validation dataset can be used for validating the golden model, %20 of population \n",
    "    + The main idea is to proof the sucess of model in the several data samples (out-of-sample) and time periods (out-of-date)\n",
    "- The concept of bias-variance tradeoff is very important strategy for deciding model complexity, number of experiments and regularization techniques\n",
    "- The importance of the predictors or input variables can be calculated for target variable, so feature elimination can be applied before training. Also, there is some metrics which can be used for automated model and feature selection: Akaike, Bayesian Information Criterion.\n",
    "- The general performance measures: Confussion Matrix, ROC (AUC-Gini), Cumulative Gain-Lift Chart, R-Squared, RMSE,  F-score\n",
    "    + __All models are wrong__, you can find a better one than the current model\n",
    "    + Model performance can not show the best performance (%100). The main objective is to find a meaningful model in terms of business, also sufficient convergence and stable performance in time\n",
    "- The general cross-validation and sampling techniques: Hold-out, Random sampling, K-fold, Bootstrap, LOOCV\n",
    "- The popular algorithms:\n",
    "    + Regression: Linear, Polynomial, Logistic\n",
    "    + k-Nearest Neighbors\n",
    "    + Decision Trees: Simple, bagging, boosting\n",
    "    + Naive Bayes\n",
    "    + Support Vector Machines\n",
    "    + Neural Networks\n",
    "- Ensemble modelling: Many different models, include different variables, can be used in the production at the same time. So, you can increase the prediction probability score for the decision making process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We aim to fit a line (linear, non-linear) which covers all data points on the coordinate system, also minimize difference between observed and predicted values\n",
    "    + x axis represents independent variable, predictor\n",
    "    + y axis represents dependent, target variable\n",
    "- We can choose a regression type depending on the target data type and x,y relation pattern:\n",
    "    + Linear Regression: x and y variable has strong relation linearly, target variable can be numerical, generally continous\n",
    "    + Logistic Regression: It is a derivation of Linear Regression. You can select logistic regression when you have a binary class target variable, ((1,0),(True,False))\n",
    "    + Polynomial Regression: If the relation is strong and non-linear pattern, you can add quadtratic, cubic forms of feature, also interaction (combination) terms of predictors\n",
    "- Linear Regression has some assumptions as follows:\n",
    "    + x and y must have strong correlation, y must show normal distribution\n",
    "    + No correlation between independent variables, multicolinearity problem\n",
    "    + The residuals must have a constant variation (heteroskedasticity), no autocorrelation and normal distribution\n",
    "- Strategies for assumptions:\n",
    "    + If the non-linearity problem occured, you could transform the independent variables using log, sqrt, square\n",
    "    + If the heteroskedasticity problem occured, you could transform the dependent variables using log, sqrt, square\n",
    "    + If the problem is multicolinearity, you can eliminate some high correlated features with others. PCA can be used to select features, also you can use penalized regression like ridge, lasso, elasticnet\n",
    "- The main objective of regression is to find optimal beta parameters (weights) which minimize total errors. \n",
    "    + Ordinary Least Square: If you have sufficient observations to calculate matrix operations (e.g. multiplication), you can solve the regression problem using OLS\n",
    "    + Gradient Descent: If you have a very large size of training set, you should iterate the solution process using chunks (mini-batch) of data or single rows to find parameters\n",
    "    + Beta: Constant values, an intercept and slope parameters for each predictors\n",
    "    + Performance metrics: RMSE, R-Squared, F-statistics\n",
    "- Related links: \n",
    "    + https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/\n",
    "    + https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/\n",
    "    + http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linreg](67.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Variation](67.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "The objective of gradient descent is to minimize cost function:\n",
    "- Hypothesis: h(theta,x) = theta[0] * x[0] + theta[1] * x[1]; x[0] = 1, bias-term\n",
    "- Loss Function: h(theta,xi) - yi; error term for each observation\n",
    "- Cost Function: Sum of loss function, (differentiable and convex function)\n",
    "    + Cost function can be defined for algorithms, linear regression, logistic regression, neural networks...\n",
    "- thetaj := thetaj - alpha (learning rate) * (derivative of cost function)\n",
    "    + alpha is constant value around 0.01, convergence speed\n",
    "    + small alpha is slow process, large alpha can be missed global optima (minimum cost)\n",
    "    + initialize thetaj randomly, update the weight for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd1](68.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd2](69.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd3](70.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd4](71.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- When you have a binary class target variable, you can apply logistic regression for training\n",
    "- There is a sigmoid (S) shape between a predictor and binarized target variable in the visual way\n",
    "- Logistic regression is a classification technique, also classification evaluation metrics can be used for model performance\n",
    "- Logistic regression is a derivation, logarithmic transformation, of the linear regression\n",
    "    + Logit transform is a link function to get the log odds-ratio of the positive (1) class\n",
    "    + You can get the proability of the positive class using an exponential function\n",
    "    + Odds-ratio: occurrence probability / not occurrence probability (for an event)\n",
    "- Finding optimal beta coefficient:\n",
    "    + Maximum Likelihood Estimate: An optimization technique\n",
    "    + Gradient Descent: Using logarithmic cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log1](72.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log2](73.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log3](74.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log4](75.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Classification\n",
    "\n",
    "There is some metrics that can be used for classification model evaluation:\n",
    "- Confussion Matrix: Precision, Recall, F1-Score\n",
    "- ROC Curve: Area Under Curve, Gini Coefficient\n",
    "- Cumulative Gain-Lift Chart\n",
    "\n",
    "You should split the data to measure the performance on taining, testing and validation, also scoring dataset:\n",
    "- You can apply the fitted model on different time period data to understand performance variation by time\n",
    "- You can split the data in 10-fold to validate the model for each %10 split in 10 iteration\n",
    "- For unbalanced distribution of target variable, can be used bootstrapping \n",
    "- Bootstrap sampling technique helps to balance distribution of target\n",
    "- Also bootstrapping is a solution to reduce sampling variance\n",
    "- Sampling techniques: Random, Stratified, bootstrapping (sample with replacement), top N\n",
    "- Cross validation: Hold-out, K-fold (sample without replacement), LOOCV, Repeated CV\n",
    "\n",
    "Unbalanced target distribution is a common problem for fraud\n",
    "- Undersampling is another solution to balance the distribution\n",
    "- You can use whole fraud cases with a randomized sub-sample of non-fraud cases\n",
    "- Oversampling means that you can simulate and duplicate fraud cases to balance the distribution\n",
    "- Undersampling is a better solution than oversampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![split](76.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![k-fold](77.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bs](78.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conf](79.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![roc1](80.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![roc2](81.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![roc3](82.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lift1](83.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lift2](84.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Underfitting (Bias): The model can show low performance both training and testing dataset.\n",
    "    + When you face with underfitting problem, you can increase complexity\n",
    "    + Model complexity can be increased by adding new features to models\n",
    "- Overfitting (Variance): The model can perform good on training while performing bad on testing set\n",
    "    + The model learn every details and patterns of training dataset\n",
    "    + The model is not generalized to other data samples\n",
    "    + The solution is to decrease complexity of the model, also increase number of observation size\n",
    "    + You can use regularization term to decrease complexity\n",
    "\n",
    "http://www.listendata.com/2017/02/bias-variance-tradeoff.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bv1](85.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bv2](86.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization, Feature-Model Selection\n",
    "- We can decrease complexity of model by indicating model parameter values, also eliminating some features\n",
    "- Regularization techniques provide automated process of model and feature selection\n",
    "    + L1: Lasso Regression provides indicating parameter values, also selecting features\n",
    "    + L2: Ridge Regression provides indicating parameter values\n",
    "    + Elasticnet\n",
    "- Akaike (AIC) and Bayesian (BIC) information criterion metrics are mostly used for model selection - penalization \n",
    "- You could use the regularization techniques when overfitting (high-variance) occured\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aic](87.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bic](88.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reg1](89.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reg2](90.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron is the single unit of neural networks, called as neuron\n",
    "- Every perceptron has an input and output layer, also sum of weighted (parameters) inputs is passed to a step unit function\n",
    "- There is some alternative activation functions as follows:\n",
    "    + Sigmoid function for binary classification\n",
    "    + Tanh function for linear problems\n",
    "    + Softmax can be used for multi-class classification\n",
    "    + Relu\n",
    "- Output layer can be one or more cell across problem types\n",
    "    + For binary class: one output cell (True/False, switch)\n",
    "    + For multi-class: more than one cell (Like hand-written digit problems, 10 class)\n",
    "    + For continous target: house prices\n",
    "- The structure is very similar with regression\n",
    "- The concept of perceptron (neuron) based on human brain structure\n",
    "\n",
    "http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perc1](91.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perc2](92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron\n",
    "XOR problem: Single neuron perceptron can not seperate the following pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xor](93.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use multi perceptron (neuron), also multi hidden layers in artificial neural networks.\n",
    "- Every single neuron's calculation process is exactly same. (weight * theta)\n",
    "- We can find optimal weights using gradient descent\n",
    "- Deep learning is a type of neural networks, which is a huge networks including lots of neurons and hidden layers\n",
    "- Types of Neural Networks:\n",
    "    + Feed Forward\n",
    "    + Backpropogation\n",
    "    + Recurrent Neural Networks, LSTM\n",
    "    + Convolutional Neural Networks\n",
    "    + Generative Adversarial Neural Networks...\n",
    "- MLP incldes the following layers:\n",
    "    + Input Layer\n",
    "    + Hidden Layer(s)\n",
    "    + Output Layer\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "\n",
    "http://www.asimovinstitute.org/neural-network-zoo/\n",
    "\n",
    "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](94.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn1](95.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn2](96.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bp](97.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even artificial and deep neural networks is considered as the state of the art machine learning tecnique, it has some disadvantages as follows:\n",
    "- Required huge data\n",
    "- Required huge computation power, mostly GPU\n",
    "- Required long training time \n",
    "- Hard to understand and interpret the model results, block-box logic\n",
    "\n",
    "https://arxiv.org/abs/1801.00631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Networks\n",
    "Can be used for language processing, machine translation, time series forecasting, image classification, pattern recognation, computer vision, gaming, audio processing and fake image detection\n",
    "\n",
    "You can find datasets, also pre-trained deep learning models for common problems like image classification using web, also you can use the pre-trained model for scoring. The method is called as \"trasfer learning\"\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-i-23d518abf531\n",
    "- http://bigtheta.io/2017/04/06/notes-on-lstms-in-finance.html\n",
    "- https://oshearesearch.com/index.php/2016/07/01/mnist-generative-adversarial-model-in-keras/\n",
    "- https://chatbotslife.com/deep-learning-in-7-lines-of-code-7879a8ef8cfb\n",
    "- https://rpubs.com/zkajdan/279967\n",
    "- https://randomekek.github.io/deep/deeplearning.html\n",
    "- https://rylanschaeffer.github.io/content/research/one_shot_learning_with_memory_augmented_nn/main.html\n",
    "- https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-3-Natural-Language-Processing\n",
    "- https://towardsdatascience.com/deep-learning-with-tensorflow-part-3-music-and-text-generation-8a3fbfdc5e9b\n",
    "- https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/\n",
    "- http://deeplearning.net/datasets/\n",
    "- https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "If you have dependent two event, you can calculate the probability of occurence the target event (posterior) using prior probability and conditional probability. In example:\n",
    "- You have many emails and you want to research the spam-probability for a new email\n",
    "- It's very hard to calculate P(A=spam | B=keywords in email) which is posterior probability\n",
    "- According to Bayes theorem, if you already know a prior probability, which is a probability of spam e-mail, you can find the spam probability for a new email\n",
    "- Prior probability can be calculated as #spam emails / #emails, P(A=spam)\n",
    "- The conditional probability can be defined as P(B=keyword | A=spam)\n",
    "- So, you can get P(A=spam | B=keywords in email) = P(B=kw1 | A=spam) * P(B=kw2 | A=spam) * .. *  P(B=kwn | A=spam) * P(A=spam)\n",
    "\n",
    "Naive Bayes algorithm is a common classification task which is used in text classification and real-time analytics and decision engines, because it can be applied and calculated easily.\n",
    "\n",
    "https://brohrer.github.io/how_bayesian_inference_works.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes1](98.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes2](99.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "- Decision Trees can be used both of classification and regression tasks, also feature selection.\n",
    "- It has a hierarchical tree structure which a user can read the results easily. Tree structure has the following objects:\n",
    "    + Root node: the starting node of tree\n",
    "    + Split nodes: the nodes are between root and leaf/terminal nodes. Split nodes keep the significant input variable and cut-off values\n",
    "    + Leaf nodes: The terminal nodes of tree. You can make decision using terminals\n",
    "- It can be called as recursive partioning and some DT algorithms are ID3, J48, C4.5, C5.0 and C&R.\n",
    "- Decision trees tend to overfitting problem and some advanaced tree algorithms are developed for this reason:\n",
    "    + Bagging trees: Bootstrap aggregating, RandomForests (Ensemble tree)\n",
    "    + Boosted tress: Adaboost, GBM, LightGBM, xgboost... (sequential, steepest process stochastic gradient descent)\n",
    "    + The objective of boosting is a find strong predictor using weak learner, the method is better than random guess\n",
    "    + Also pruning tree, decreasing tree depth are other techniques to stop overfitting \n",
    "- Splitting criteria for tree:\n",
    "    + Entropy based Information Gain\n",
    "    + Gini\n",
    "    + Chi-square\n",
    "- How to interpret the results/metrics:\n",
    "    + Support is the #observations in the total population for any node\n",
    "    + Confidence is the #positive class rate in the #total cases in a node\n",
    "- The initial settings of decision trees:\n",
    "    + Number of tree depth\n",
    "    + Splitting criteria, gain, gini..\n",
    "    + Mininum percent in a split\n",
    "    + Mininum percent in a node (You need to decide the final class if the terminal has sufficient observations)\n",
    "    \n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt1](108.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt2](109.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt3](110.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt3](111.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt3](112.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt3](113.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dt3](114.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "SVMs can be used for regression and classification, also one-class SVM algorithm can be used in anomaly detection in unsupservised learning concept.\n",
    "\n",
    "- SVM can find decision boundaries (classify or fit a line) using hyperplanes with a margin for given data points with/without target\n",
    "- It tries to find maximum margin length to fit optimal model\n",
    "- The data points, which are boundaries for margins, are called as support vectors\n",
    "- SVM can be used for binary and multi-class classification, logistic regression is a common task for a binary classification\n",
    "- SVM model results are not readable\n",
    "- You can tune the models by finding optimal values for hyperparameters like complexity factor, gamma (for gaussian)...\n",
    "- Support vector machines have generally two classifier techniques called as kernels:\n",
    "    + Linear is default kernel\n",
    "    + You can fit a model for non-linear patterns using Gaussian kernel (RBF)\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
    "https://yunhaocsblog.wordpress.com/2014/07/27/the-effects-of-hyperparameters-in-svm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm1](100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm2](101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm3](102.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm4](103.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm4](104.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "k-NN is a well-known classification task which is based on neighborhood (distance) logic. You need to give a parameter k, the algorithm tries to classify the single point looking for nearest data points' (#k) classes.\n",
    "\n",
    "- k-NN is also used for regression tasks\n",
    "- Euclidean distance is generally used for calculation of distance\n",
    "- The k-NN is a \"lazy learning\" system and it can not be preferred for large-scale problems because of memory management\n",
    "    + It can be a useful tool for missing value imputation or some pre-processing tasks instead of modelling\n",
    "- It's not easy to choose optimal k value. Can be decided to it based on trials.\n",
    "    + If you choose a small k-value, i.e. 1, your model will tend to classify the new cases using the most frequent class level (overfitting problem, high-variance)\n",
    "    + If you choose a big k-value, let's say 10, your model will be more robust but can be seen underfitting problem (high bias)\n",
    "    + Optimal k-values depends on your dataset, you need to find optimal k with your trials\n",
    "- There is some other distance metrics can be used like Minkowski, Hamming, Manhattan... \n",
    "- Can be represented as Voronoi diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"knn1\"](105.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn2](106.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn3](107.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "The learning system can be used for clustering, association, anomaly detection, recommendation and social network analysis\n",
    "\n",
    "You can apply for explorative data analysis and hypothesis investigation rather than predictive modelling.\n",
    "\n",
    "There is no common testing methods for unsupervised learning, but you can apply some techniques like silhouette test or cross validatation your model using other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering\n",
    "\n",
    "It is a common task for clustering, customer segmentation/profiling also can be used for anomaly detection.\n",
    "\n",
    "You need to define cluster size (k) initially. Can be used elbow graph to choose optimal k\n",
    "\n",
    "The algorithm tries to seperate data points into k groups which are the nearest observation in group (inter-class) and farthest distance between groups (intra-class)\n",
    "\n",
    "The distance metrics, euclidean, cosine similarity, can be used\n",
    "\n",
    "You can use hierarchical (tree structure, dendogram representation) k-means\n",
    "\n",
    "Outlier can lead k-means clustering, also initial potins are randomly selected. So you can get different results for each execution of the algorithm with the same data and same settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![km1](115.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![km2](116.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![km3](117.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Simiilarity\n",
    "In vector space model, you can find similarity for two different vectors using cosinus.\n",
    "\n",
    "You can use the cosine similarity metric in many unsupervised learning technique like document clustering, collaborative filtering.\n",
    "\n",
    "You just need to represent the variables as vectors like word, item list, word list..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cs1](118.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cs2](119.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cs3](120.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cs4](121.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Based Scan Clustering\n",
    "\n",
    "Can be used for outlier/anomaly detection with local outliers\n",
    "\n",
    "You need to set the parameters: eps (the round metric for distance/density), minpts (#observations in given round)\n",
    "\n",
    "Core point is the center of the cluster/round, border is a nearest point for cluster and outlier is the farthest point from core\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![db1](122.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![db2](123.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![db3](124.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization\n",
    "\n",
    "EM can be used for hidden information and component extraction\n",
    "\n",
    "Assume that you have a dataset except gender variable\n",
    "\n",
    "You can use EM clustering to find most likely gender using some behavioral known features. These features must show significant distribution for gender\n",
    "\n",
    "EM algorithms is an iterative process until convergence and has two steps as follows:\n",
    "- E step: Set (Init once) distr. parameters (mu,sigma for gaussian) and update hypothesis\n",
    "- M step: Update distr. parameters and set hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![em1](125.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![em2](126.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![em3](127.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Partitioning\n",
    "\n",
    "Grid-based clustering can be used for spatial clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![oc](128.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection - One-class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ocsvm1](129.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ocsvm2](130.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori\n",
    "\n",
    "Associal rules/apriori is a useful task to investigate the most fruquent item-sets and rules in a basket/session\n",
    "\n",
    "It can be used for cross-selling and click-stream data analysis to explore session-based menu associations\n",
    "\n",
    "The algorithm expects rule length size, min support and min confidence level initially\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ap1](131.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ap2](132.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "CF is a useful tool for recommendation engine. The algorithm tries to find similar user groups using collaboration of user's behaviors and interests like rating, voting, offer acceptance\n",
    "\n",
    "There is 3 types of recommender system: \n",
    "- Content-based filtering: using product contents, text processing and clustering\n",
    "- Memory-based CF: using similarity metrics like cosine similarity, jackard index, also neighborhood\n",
    "- Model-based CF: using matrix factorization/decomposition techniques like PCA, SVD, NMF, ALS\n",
    "\n",
    "If you have not ratings (explicit database) for products, you can create your proxy feature (using implicit databae) like RFM score or offer acceptance result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cf1](134.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cf2](135.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cf3](136.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cf4](137.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Network Analysis\n",
    "\n",
    "To find clusters and roles using linked/connected data\n",
    "\n",
    "Can be implemented on graph databases like Neo4j easily\n",
    "\n",
    "Closeness, Betweennes, shortest path, pagerank can be used for analytical scenarios\n",
    "\n",
    "Graph data can exist as property graph (vertice-edge) or semantic graph (ontology, OWL, RDF) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](138.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
