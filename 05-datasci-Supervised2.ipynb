{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADVANCED ANALYTICS 2018\n",
    "_Author: Ali Ã‡abukel_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "- We aim to fit a line (linear, non-linear) which covers all data points on the coordinate system, also minimize difference between observed and predicted values\n",
    "    + x axis represents independent variable, predictor\n",
    "    + y axis represents dependent, target variable\n",
    "- We can choose a regression type depending on the target data type and x,y relation pattern:\n",
    "    + Linear Regression: x and y variable has strong relation linearly, target variable can be numerical, generally continous\n",
    "    + Logistic Regression: It is a derivation of Linear Regression. You can select logistic regression when you have a binary class target variable, ((1,0),(True,False))\n",
    "    + Polynomial Regression: If the relation is strong and non-linear pattern, you can add quadtratic, cubic forms of feature, also interaction (combination) terms of predictors\n",
    "- Linear Regression has some assumptions as follows:\n",
    "    + x and y must have strong correlation, y must show normal distribution\n",
    "    + No correlation between independent variables, multicolinearity problem\n",
    "    + The residuals must have a constant variation (heteroskedasticity), no autocorrelation and normal distribution\n",
    "- Strategies for assumptions:\n",
    "    + If the non-linearity problem occured, you could transform the independent variables using log, sqrt, square\n",
    "    + If the heteroskedasticity problem occured, you could transform the dependent variables using log, sqrt, square\n",
    "    + If the problem is multicolinearity, you can eliminate some high correlated features with others. PCA can be used to select features, also you can use penalized regression like ridge, lasso, elasticnet\n",
    "- The main objective of regression is to find optimal beta parameters (weights) which minimize total errors. \n",
    "    + Ordinary Least Square: If you have sufficient observations to calculate matrix operations (e.g. multiplication), you can solve the regression problem using OLS\n",
    "    + Gradient Descent: If you have a very large size of training set, you should iterate the solution process using chunks (mini-batch) of data or single rows to find parameters\n",
    "    + Beta: Constant values, an intercept and slope parameters for each predictors\n",
    "    + Performance metrics: RMSE, R-Squared, F-statistics\n",
    "- Related links: \n",
    "    + https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/\n",
    "    + https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/\n",
    "    + http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/\n",
    "    \n",
    "    \n",
    "### Gradient Descent\n",
    "The objective of gradient descent is to minimize cost function:\n",
    "- Hypothesis: h(theta,x) = theta[0] * x[0] + theta[1] * x[1]; x[0] = 1, bias-term\n",
    "- Loss Function: h(theta,xi) - yi; error term for each observation\n",
    "- Cost Function: Sum of loss function, (differentiable and convex function)\n",
    "    + Cost function can be defined for algorithms, linear regression, logistic regression, neural networks...\n",
    "- thetaj := thetaj - alpha (learning rate) * (derivative of cost function)\n",
    "    + alpha is constant value around 0.01, convergence speed\n",
    "    + small alpha is slow process, large alpha can be missed global optima (minimum cost)\n",
    "    + initialize thetaj randomly, update the weight for each iteration\n",
    "    \n",
    "### Logistic Regression\n",
    "- When you have a binary class target variable, you can apply logistic regression for training\n",
    "- There is a sigmoid (S) shape between a predictor and binarized target variable in the visual way\n",
    "- Logistic regression is a classification technique, also classification evaluation metrics can be used for model performance\n",
    "- Logistic regression is a derivation, logarithmic transformation, of the linear regression\n",
    "    + Logit transform is a link function to get the log odds-ratio of the positive (1) class\n",
    "    + You can get the proability of the positive class using an exponential function\n",
    "    + Odds-ratio: occurrence probability / not occurrence probability (for an event)\n",
    "- Finding optimal beta coefficient:\n",
    "    + Maximum Likelihood Estimate: An optimization technique\n",
    "    + Gradient Descent: Using logarithmic cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linreg](67.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Variation](67.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd1](68.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd2](69.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd3](70.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gd4](71.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log1](72.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log2](73.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log3](74.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log4](75.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
